# -*- coding: utf-8 -*-
"""Diamond Price Prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sg365zD-PpFQfj4JusEJVHtNyuTslQyP
"""

import pandas as pd

df = pd.read_csv("/content/diamonds.csv")

df.head()

df.isnull().sum()

df.shape

df = df.drop(columns=["Unnamed: 0"])

df.head(3)

df.rename(columns={"x":"length", "y":"width", "z":"depth"},inplace=True)

x = df.drop(columns=["price"])
y = df["price"]

x.head(3)

y.head(3)

cat_variables = x.select_dtypes(include="object")
cat_variables.columns

for column in cat_variables.columns:
  print(x[column].value_counts())
  print("="*10)

x.head()

cut = {"Fair": 1, "Good": 2, "Very Good": 3, "Premium": 4, "Ideal": 5}
color = {"J": 7, "I": 6, "H": 5, "G": 4, "F": 3, "E": 2, "D": 1}
clarity = {"I1":1, "SI2":2, "SI1":3, "VS2":4, "VS1":5, "VVS2":6, "VVS1":7, "IF":8}

x["cut"]=x["cut"].map(cut)
x["color"]=x["color"].map(color)
x["clarity"]=x["clarity"].map(clarity)

x.head(3)

# Feature scaling # feature scaling also done after splitting a data into train and test
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

# Model training
# XGBoost, svm, knn
# XGBoostRegressor , KNeighborsRegressor , supportvectorRegressor

from sklearn.neighbors import KNeighborsRegressor
knn = KNeighborsRegressor()
knn.fit(x_train, y_train)

ypred=knn.predict(x_test)

from sklearn.metrics import mean_squared_error

mse_knn = mean_squared_error(y_test, ypred)

(mse_knn)**0.5

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# pip install xgboost

from xgboost import XGBRegressor
xgb_reg = XGBRegressor()
xgb_reg.fit(x_train, y_train)





from google.colab import drive
drive.mount('/content/drive')

"""# New Section"""